今天分享的是属性图聚类：一种深度注意力嵌入方法，发表在 2019 年 IJCAI

首先看为什么要研究图聚类：随着各种技术的发展，数据的形式不再局限于欧几里得类型，更多的是非欧类型的数据，如图的形式。许多现实世界的任务都依赖于图数据挖掘技术。而图数据的复杂度指节点连边等等结构性质都给这些任务带来巨大的挑战，而其中一个热门的问题就是图聚类

这篇文章研究的对象是属性图，给定节点集，边集，邻接矩阵由前两者定义，如果两个节点存在连边，那么邻接矩阵对应元素为1，否则为0.属性值是向量的形式，与每个节点相关。

那么图聚类的目的就是把给定图G里的节点分成 k 个互不相连的节点组G1G2GK

那么和一般聚类类似的是类内距离尽量小，类间距离尽量大，属性图聚类同一类除了结构上要接近，属性值也要相似。

那么属性图聚类的关键问题就在于如何捕捉结构关系和内容信息

随着DL的进步，学习低维表示变成了常用的技巧。同样也有关于属性图同时对内容和结构进行嵌入的做法。

学习了结构以及属性的嵌入之后，先前的工作就在此基础上进行聚类

而自编码器正是一种主流方法

但是这些方法都是两步的，即先学习embedding，然后在此基础上进行聚类。

这样可能造成的问题就是学习到的 embedding 对于之后的图聚类来说并不是最适合的。

那么本文的做法是想让学习embedding和聚类一同优化，以任务为导向以提升聚类的效果。

本文的算法叫做 DAEGC 深度注意力嵌入图聚类

主要分为两个部分，一个是用于学习embedding 的图注意力自编码器，另一个是自训练聚类。

图注意力自编码器的输入是属性值和图结构，通过最小化重构损失来学习embedding

自训练聚类则是在embedding的基础上执行聚类，然后最小化聚类损失来优化聚类性能。

在本文这两个部分将会一起进行优化。

先看第一个部分 图注意力自编码器

文章将结构A和属性X内的信息同时输入到图编码器中进行embedding的学习，这个图编码器使用的是图注意力网络 GAT 的一个变体

它的想法是通过对每个节点自身和邻居的信息结合起来，邻居的定义方式就带有了结构信息，邻居带有的属性值则体现内容信息。一种直观的做法就是将节点和邻居的信息加权求和

这个公式就是第一部分的核心。

z就是我们需要的embedding，Ni是节点i 的邻居集合，阿尔法就是常说的注意力系数，表示邻居j对于节点i的重要程度，W是权重矩阵，是一个线性变换过程，加入这个的原因是为了获得足够的表达能力。sigma是非线性激活函数。

这个式子是单独的一层

那么问题的关键就落在了注意力系数，因为我们需要结构信息+属性信息，要包含这两者就需要对注意力系数进行修改。

注意力系数阿尔法ij在原先GAT的基础上增加了一个拓扑距离

原先的GAT实质上就是此处属性值的处理，对当前节点i和其中一个邻居j进行线性变换后，两者进行运算，此处使用的运算是拼接，然后经过一层神经网络得到cij

从拓扑距离上看，原先的GAT使用的拓扑信息是一阶邻近，也就是直接与节点i相连的点即为邻居，此处考虑更高阶的邻近性，B为转移矩阵，当存在边时，对应的元素为节点度数分之一，否则为0，然后取指数，实际效果应该和n阶可达类似。

此处的Mij表示节点i和节点j的拓扑相关性。定义好了这两个之后，就可以计算阿尔法ij了，

原先的GAT如上式所示，此时的Ni是一阶邻近，下式则是改进的注意力系数，此时的Ni是n阶邻近，除此之外，文章还直接把拓扑信息直接乘到属性值上再通过leakyRelu。

得到了注意力系数，就可以计算出节点i对应的embedding，此处用了两层的注意力层，两层各自有不同的权重矩阵，这个encoder就是本文的第一部分。

那么前面说到单独的第一部分是可以最小化重构损失来进行优化的，嗯对啊--切ppt

那么这里使用一个简单的内积函数，通过sigmoid函数对邻接矩阵进行重构，可以说如果节点i和节点j的属性以及拓扑距离足够近，也就说明其嵌入的相似性越高，那么这个内积也就越大，两者之间存在连边的可能也越大。当然还可以重构别的东西，比如属性或者是两者兼得。

那么重构损失也很简单就是每个节点在两个邻接矩阵内的连接情况的对比，具体的损失定义没有给出

第二部分则是自优化过程，首先引入一个概念 soft assignment 软分配，意思是样本i分配给聚类j 的概率，概率高则称为 confident assignment。

通常使用t分布来度量样本和聚类中心嵌入的相似性。这一做法在于能够适用于不同尺度的聚类，经过证明也是很方便计算的。第二部分要做的就是尽量学习那些具有高概率的软分配。所以这里借助一个目标分布，让我们需要的Q尽可能地接近这个目标分布P，所以这个目标分布P如何定义就很重要了。

提出这一做法的文章里提到要满足三点：一是加强预测能力，也就是提升聚类的纯度也就是正确聚类的数量除以总数量。第二点是更注重于高概率的数据点，第三点是进行归一化，防止大的分类出现影响嵌入空间。

具体的定义方式是将q进行平方化之后归一化，分子上除的对i求和是聚类中心j的边缘概率。关于这一部分有一个专门的分析，说明这个P能够达成前面所提到的三点要求。这个聚类损失起作用的前提是Q分布中高信念指派大部分是正确的，因为是对高信念assignment的增强，哪怕是错误的，所以要对这个聚类损失优化的时候要有比较好的初始聚类。实际的实验中使用k-means初始化聚类中心

有了这两个部分，将两个损失进行tradeoff，引入一个gamma来控制，这是总的损失函数。

具体到算法：先通过最小化重构损失来更新自编码器，得到预训练的编码器，然后根据编码器得到的嵌入进行k-means选取出事的聚类中心 miu，L 控制迭代次数，进入循环根据学生t分布计算样本嵌入z和聚类中心嵌入miu的相似性，为了保持自训练的稳定性，设置了迭代更新的区间，例如每过5次进行目标分布的更新，这和DQN等方法是一样的做法，然后根据P 和 Q进行KL散度的计算即聚类损失。

最小化聚类损失和重构损失构成的损失L来对整个框架进行优化。

完成一定循环次数之后根据Q来进行聚类。

这就是DAEGC整个算法的过程。

然后在引文网络数据集上的性能超过其他算法，实验就不讲了。
