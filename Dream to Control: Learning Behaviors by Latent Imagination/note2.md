今天分享的文章是 Dream to Control，通过隐想象力来学习行为，文章作者主要是谷歌方面，正式发表在 ICLR 2020

先来看摘要，已经学习好的世界模型总结了一个智能体的经验，以促进学习复杂行为。
利用深度学习方法从高维的感官输入中学习出世界模型正在变得可行，随之出现了许多潜在的方法能从世界模型中推导出行为。
文章提出了 Dreamer 梦想家，纯粹通过隐想象力来解决图像的长跨度任务。
学习到的世界模型提供了一个紧凑的状态空间，根据这一模型想像出各种轨迹，再用学习到的状态值函数的梯度来学习行为。
在20个具有挑战性的视觉控制任务上，Dreamer在数据效率、计算时间和最终性能上都超过了现有的方法。

第一节 introduction 第二节 利用世界模型实现控制，第三节通过隐想象力来学习行为，第四节是学习隐动态，也就是世界模型，第五节相关工作，第六节是实验。

introduction 一个足够聪明的智能体应该能够在复杂环境中达成目标，即便从来没有遇到过完全相同的状况。这样的能力要求在过去经验的基础上建立对世界的一个表示，world model 这里暂且称为世界模型，就是一种显式的参数化模型用于表示智能体对世界的了解，以此进行对未来的预测。

隐动态模型则是将观察 observation 抽象到一个紧凑的状态空间，和直接在原来的状态空间相比，这种方式占用的内存更少，也就是其产生预测轨迹的能力更强。

从动态模型中得到行为有很多种方式，例如参数化策略最大化累计奖励，或者在线规划。一般而言累计奖励是有一个 horizon，也就是范围的，也就是从当前状态往前看多少步，固定这个步数的话可能会造成智能体的短视。还有一些方法使用的是无导数优化。此处不展开，就是说存在短视行为。

那么 Dreamer ，是一个单纯使用隐想象力学习到远见行为的智能体。

注意到第二节开头将视频控制问题建模为 POMDP，部分可观察马尔科夫决策过程，**POMDP** 简单地说就是给 MDP 增加了一个观察模型 以状态st为条件得到观察ot的概率分布。

Dreamer 实际上是分为三个过程：利用旧的经验学习世界模型，也就是隐动态模型；根据世界模型做的预测进行行为的学习，也就是隐想象力；然后在环境中执行学习到的行为收集新的经验。可以看到图 3图里还有奖牌代表reward，奖杯表示的是价值 value，value是用于考虑超出 horizon 的奖励，也就是行为具有远见性的原因。

前两个过程会依照顺序展开讲

隐动态模型也就是世界模型分为三个部分，表示模型，转移模型以及奖励模型。注意到公式1中 p 和 q。p表示的是真实环境也就是真实状态的状态概率分布，q则是隐动态空间内的概率分布。**模型使用了非线性 Kalman filter 卡尔曼滤波，隐状态空间模型或隐马尔科夫 HMM**

我们先看第四节，学习隐动态模型。第一段提到学习表示的几种目标，这里给出了三种方式：奖励预测、图像重构以及对比估计。

奖励预测就比较直观，以真实奖励作为label即可。不过遇到稀疏奖励还是比较艰难。图像重构是和奖励预测一起做的，我们可以看一个 gif。这个gif里出现了奖励预测和图像重构，如 gif 所示我们有从过去经验中得到的一系列的图像观测o1o2o3，然后o1经过encoder计算出一个隐状态s1，隐状态结合动作a1和下一个观测o2经过encoder得到下一个隐状态s2，然后预测奖励r_hat，重构原始图像o_hat

Dreamer 使用了 PlaNet 世界模型，PlaNet 全称是 Deep Planning Network，18年的一篇文章，Dreamer主要用到的是其中的 RSSM 循环状态空间模型，是RNN 和 SSM 状态空间模型的结合。他是这么做的，对于转移模型由神经网络生成下一个状态的均值和方差，并从高斯模型中进行采样，如果每一步的转移模型都是准确的，那逐步估计也会准确，但实际上做不到，每一步都是随机采样那么经过很多步之后就会丢失掉前面的信息。所以文章还引入了一个确定性的部分ht，由一个确定性的函数给出。

回到文章看到公式9给出了三个参数化模型，又加入观测模型，这个是用于图像重构的。图10给出了这一过程的损失函数，根据变分信息瓶颈推导得到的损失函数。【变分信息瓶颈】

信息瓶颈认为面对一个任务，试图用最少的信息来完成，也就是用最低的成本来完成任务。而这也意味着能得到更好的泛化能力，比如说有一群客户，每个客户都定制一个方案这样的成本是很高的，而如果能找出一个具有普适性的方案，那么在此基础上进行微调，那么成本就低很多。所以说一个成本最低的方案意味着能够找到一些具有普适性的规律和特性，这就意味着泛化性能。

那么放到深度学习中，要如何体现这一点？就是这里提到的VIB 变分信息瓶颈，假设我们有一个分类任务，数据对是xi yi，那么两步走，第一步将x编码为隐变量z，然后分类器把z识别为类别y。然后我们在z上加一个瓶颈beta，这样就像是一个沙漏，进入的信息量很多但出口只有beta这么大，也就是说不允许流过z的信息量超过beta，信息经过了瓶颈之后还有分类任务要完成，所以流过的信息要能够达到完成这一任务的目的，也就是想办法让最重要的信息通过瓶颈。这就是信息瓶颈的原理。

实际的操作是以互信息为指标来度量通过的信息量，分类任务的loss加上一个惩罚项，也就是限制流过z的信息量小于beta，也就是大于beta的时候会出现一个正的惩罚项，但往往我们不知道设置beta为多少，那就去掉，也就是让信息量越小越好。

那么和PlaNet 的区别在哪？这里使用了一个最为常用的框架，叫做model-predictive control（MPC）。简单说来就是每一步都往后面模拟很多步，然后选取一个收益最高的方案，然后只采取一步的行动；等到下一步的时候再重新规划并且选取新的一步；即并不是规划出一串行动之后开环地做着一串，而是规划一串只走一步。

说完了这一部分回到第三节，现在我们训练好了一个世界模型，就使用这个模型来生成轨迹，Dreamer使用AC框架来学习动作。动作模型尝试预测动作，而价值模型则估计期望的奖励。都是普通的神经网络带有不同的参数，动作模型输出的是高斯经过双曲正切变换得到的概率分布。动作都是从分布中采样得到，为了得到有效的梯度就需要使用重参数。从本质上看重参数就是一种积分变换，VAE也常用这种技巧。

为了学习动作和价值模型，需要对假想轨迹的状态值进行估计，估计方式有很多种，简单的直接H步求和，或者是k步bootstrap。前者无偏但是方差高，后者有偏但是方差低。所以两者指数加权进行一个tradeoff。那么计算假想轨迹所有状态的状态值估计之后，开始更新两个模型。动作模型的目标是选择那些使得轨迹的状态值更高的动作，价值模型则是回归状态值估计。所以前者是最大化累计状态值估计，后者是SSE。在学习行为的时候世界模型是固定的。

第三节说完就剩下实验

文章在20个不同任务的标准基准上对Dreamer进行了评估，包括连续的动作和图像输入。任务包括平衡和捕捉物体，以及各种模拟机器人的运动。这些任务旨在对智能体提出各种挑战，

文章将Dreamer的性能与PlaNet、流行的无模型代理A3C以及当前在此基准上最好的无模型代理D4PG的性能进行了比较，基于模型的智能体可以在500万帧以下高效学习，模拟时间为28小时。无模型代理学习更慢，需要1亿帧，对应于23天内的模拟。

在20个任务的基准上，Dreamer的平均得分为823分，高于最佳无模型代理（D4PG），而786分，同时从20倍的环境交互中学习。而且，它超过了几乎所有任务中PlaNet的最终性能。训练Dreamer 16小时的计算时间小于其他方法所需的24小时，四个代理的最终性能如图6

除了在连续控制任务上的主要实验外，还通过将Dreamer应用于具有离散动作的任务来演示Dreamer的通用性。 为此选择了雅达利游戏和DeepMind Lab，它们需要反应和远见行为，空间意识，以及对视觉上更多样化的场景的理解。

结论
