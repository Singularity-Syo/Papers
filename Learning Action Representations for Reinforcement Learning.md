# Learning Action Representations for Reinforcement Learning

标题：Learning Action Representations for Reinforcement Learning

作者：[Yash Chandak] [Georgios Theocharous] [James E. Kostas] [Scott M. Jordan] [Philip S. Thomas]

发表：ICML 2019

链接：

## Abstract

大多数无模型强化学习方法利用状态表示 (嵌入) 进行泛化，但是要么忽略动作空间中的结构，要么假设该结构是先验的。 
文章展示了如何将策略分解为在动作表示的低维空间中执行动作的组件和将这些表示转换为实际动作的组件。 
这些表示通过允许智能体推断与已经采取的动作类似的动作的输出，改进了对大型的、有限的动作集的泛化。 
文章给出了一种既能学习又能使用动作表示的算法，并给出了算法收敛的条件。
在大规模的实际问题上验证了该方法的有效性。

## 1. Introduction

强化学习 (RL) 方法已经成功地应用于许多简单的、基于游戏的任务中。 
然而，对于许多现实世界中涉及决策的问题，它们的适用性仍然有限。 
一个原因是，许多具有重大人类影响的现实世界问题都涉及从众多可能的选择中选择一个决定。 
例如，

- [ [Jiang et al.](), [2017]() ] 使用各种交易策略以最大化金融的长期投资 (portfolio) 组合价值；
- [ [Glavic et al.](), [2017]() ] 通过调节大型电力系统中所有机组的电压水平来提高容错性；
- [ [Sidney et al.](), [2005]() ] 个性化辅导系统用于推荐大量教程中的系列视频。

因此，开发对实际问题有效的 RL 算法是很重要的，因为在现实问题中，可能的选择数量很多。

本文考虑了创建对具有大动作集的问题有效的 RL 算法的问题。 
现有的 RL 算法通过学习状态的表示或嵌入 (例如，使用神经网络中的线检测器或卷积层) 来处理大的状态集 (例如，由像素组成的图像)，这允许智能体使用状态表示而不是原始状态进行推理和学习。 
本文将这个想法扩展到动作集：我们建议学习动作的表示，这允许智能体通过在动作表示空间中决策，而不是通过原始的大量可能的动作来进行推理和学习。 
此设置如 **图 1** 所示，其中内部策略 (internal policy) $\pi_i$ 在动作表示空间中操作，函数 $f$ 将这些表示转换为实际动作。 
将 $\pi_i$ 和 $f$ 称为总体策略 (overall policy) $\pi_o$。

最近的工作表明了与使用动作嵌入相关的好处 [ [Dulac-Arnold et al.](),[2015]() ]，特别是它们允许对动作进行泛化。 
对于现实世界中有数千种可能的 (离散的) 动作的问题，这种泛化可以显著加快学习速度。 
然而，这项先前的工作假设提供了固定的和预定义的表示。 
本文提出了一种通过使用观察到的转换来自主学习动作集的底层结构的方法。 
该方法既可以从头开始学习动作表示，又可以在已有动作表示的基础上进行改进。

本文提出的方法的一个关键部分是它将学习动作表示 (学习 $f$) 的问题变成监督学习问题，而不是强化学习问题。 
这是可取的，因为监督学习方法往往比强化学习算法学习更快、更可靠，因为它们可以获得指导性反馈 (instructive feedback) 而不是评估性反馈 (evaluative feedback) [ [Sutton & Barto.](), [2018]() ]。 
本文所提出的学习过程通过基于动作对状态的影响的相似性来对齐动作，从而利用动作集中的结构。 
因此，在学习好的动作表示空间中进行操作的策略的更新，将在采取动作之后接收到的反馈泛化到具有类似表示的其他动作。 
此外，本文证明了将监督学习 (针对 $f$) 和强化学习 (针对 $\pi_i$) 组合成的一个更大的 RL 智能体保持了策略梯度算法提供的几乎肯定的收敛保证 [ [Borkar & Konda.](), [1997]() ]。

为了对提出的方法进行经验性评估，使用来自广泛使用的商业应用的数据研究了两个真实世界的推荐问题。 
在这两个应用中，在每个时间步都可以给出数千个可能的推荐 (例如，建议用户接下来观看哪个视频，或者在多媒体编辑软件中向用户推荐下一个工具)。 
实验结果表明，本文提出的系统能够通过快速可靠地学习动作表示来显著提高这些应用的现有方法的性能，这些动作表示允许在具有大量离散的可能动作的动作集上进行有意义的泛化。

本文的其余部分按以下顺序组织：RL 的背景、相关工作和以下主要贡献：

- 一种利用动作表示的新的参数化，称为总体策略 (overall policy)。 本文证明了对于所有最优策略 $\pi^∗$，这个新策略类存在且等价于 $\pi^*$ 的参数。
- 整体策略和内部策略之间策略梯度更新的等价性证明。
- 一种用于学习动作表示的监督学习算法 (**图 1** 中的 $f$)。 该过程可以与任何现有的策略梯度方法相结合来学习总体策略。
- 该算法几乎肯定是渐近收敛的证明，它扩展了 Actor-Critics 的现有结果[ [Borkar & Konda](), [1997]() ]。
- 使用从推荐系统收集的实际数据，在具有数千个动作的真实世界领域上进行实验的结果。

## 2. Background

我们考虑被建模为具有离散状态和有限动作的离散时间马尔可夫决策过程 (MDP) 的问题。 一个 MDP 由一个元组 $\mathcal{M}=(\mathcal{S},\mathcal{A},\mathcal{P},\mathcal{R},\gamma,d_0)$ 表示。 
$\mathcal{S}$ 是所有可能状态的集合，称为状态空间，
$\mathcal{A}$ 是有限的动作集合，称为动作集。 
虽然我们的符号假设状态集是有限的，但是我们的主要结果推广到具有连续状态的 MDP。 
在这项工作中，我们将重点限制在具有有限动作集的 MDP 上，并且 $|\mathcal{A}|$ 表示动作集的大小。 
随机变量 $S_t\in \mathcal{S}, A_t\in\mathcal{A},R_t\in\mathbb{R} 表示在时间 $ $t\in\{0,1,\cdots \}$ 的状态、动作及奖励。 
我们假设 $R_t\in[−R_{max},R_{max}]$ 对于某些有限的 $R_{max}$。 
第一个状态 $S_0$ 来自初始分布 $d_0$，并且定义了奖励函数 $R$，使得对于所有 $s\in\mathcal{S}$ 和 $a\in\mathcal{A}$，$\mathcal{R}(s，a)=E[R_t|S_t=s，A_t=a]$。
此后，为简明起见，我们写 $P$ 来表示概率和概率密度，并且在写概率和期望时，写 $s,a,s&#39;,e$ 来表示各种集合的元素和事件 $S_t=s,A_t=a,S_{t+1}=s&#39;,E_t=e$ (稍后定义)。 
$s,a,s&#39;,e$ 的含义应该从上下文中清晰可见。 
奖励折扣因子由 $\gamma\in[0，1]$ 给出。 
$\mathcal{P}$ 是状态转移函数，使得 $\forall s,a,s&#39;,t,\mathcal{P}(s,a,s&#39;):=P(s&#39;|s,a)$。

策略 $\pi:\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$ 是针对每个状态的动作的条件分布：$\pi(a,s):=P(A_t=a|S_t=s)$。
虽然 $\pi$ 只是一个函数，但我们写 $\pi(a|s)$ 而不是 $\pi(a,s)$ 来强调它是条件分布。

对于给定的 $\mathcal{M}$，智能体的目标是找到一种策略，使预期的折扣未来回报之和最大化。 对于任意策略 $\pi$，对应的状态动作值函数为 $q^\pi(s,a)=E[\sum_{k=0}^\infty R_{t+k}|S_t=s,A_t=a,\pi]$，其中对 $\pi$ 的条件表示对所有 $S_{t+k}$ 和 $A_{t+k}$ 有 $A_{t+k}\sim\pi(\cdot|S_{t+k})$，其中 $k\in[t+1,\infty)$。

状态值函数为 $v^\pi(s)=E[\sum_{k=0}^\infty\gamma^t\R_{t+k}|S_t=s,\pi]$。 
从贝尔曼方程推导出 $v^\pi(s)=\sum_{a\in\mathcal{a}}\Pi(a|s)q^{\pi}(s,a)$。
最佳策略是 $\pi^∗\in\arg\max_{\pi\in\Pi}E[\sum_{t=0}^\infty \gamma^t R_t|\pi]$，其中 $\Pi$ 表示所有可能策略的集合，$v^*$ 是 $v^{\pi^*}$ 的缩写。

## 3. Related Work

在这里，文章总结最相关的工作，并讨论它们与本工作的关系。

**Factorizing Action Space**
分解动作空间

为了减小大型动作空间的大小，[ [Pazis & Parr](), [2011]() ] 考虑用二进制格式表示每个动作，并学习与每个位相关联的值函数。 
一种类似的基于二进制的方法也被用作学习具有大动作集的 MDP 的最优策略的集成方法[ [Sallans & Hinton](), [2004]() ]。 
对于规划问题，[ [Cui & Khardon](), [2016]();[2018]() ] 展示了如何使用基于状态行动值函数的符号表示的基于梯度的搜索来解决可伸缩性问题。 
最近，有研究表明，在 Atari 2600 游戏 [ [Bellemare et al.](), [2013]() ] 上，当将动作纳入其主要类别时，可以取得更好的表现 [ [Sharma et al.](), [2017]() ]。 
所有这些方法都假定提供了手工制作的原始动作的二进制分解。 
为了处理可能具有潜在的连续表示的离散动作，[ [Van Hasselt & Wiering](), [2009]() ] 使用具有连续动作的策略梯度，并选择最近的离散动作。
这项工作由 [ [Dulac-Arnold et al.](), [2015]() ] 推广。对于较大的域，他们在其中执行动作表示时向上查看，类似于我们的方法。 
然而，他们假设动作的嵌入是给定的，这是先验的。 
最近的工作还表明，如何使用专家演示中的数据来学习行动表征[ [Tennenholtz & Mannor](), [2019]() ]。
我们提出了一种方法，可以在没有先验知识的情况下学习动作表示，或者进一步优化现有的动作表示。 
如果没有可用的先验知识，我们的方法会自动从头开始学习这些表示。

**Auxiliary Tasks**
辅助任务

先前的研究经验表明，监督学习的目的是从其他元组中预测转换元组 $(s,a,r,s_0)$ 的一个组成部分，可以作为学习状态表征的辅助方法 [ [Jaderberg et al.](), [2016]()] ];[ [Fran cäois-Lavet et al.](), [2018]() ] 或获得内在奖励 [ [Shelhamer et al.](), [2016]() ]; [ [Pathak et al.]()., [2017]() ]。
我们展示了如何使用使用类似的损失函数学习的动作表示模块来分解整个策略本身。

**Motor Primitives**
运动基元

神经科学的研究表明，动物将它们的计划成中级抽象，而不是每个动作所需的确切的低级运动控制 [ [Jing et al.](),[2004]() ]。 
这样的行为抽象形成了运动控制的基础，通常被称为运动原语基元 (Motor Primitives) [ [Lemay & Grill](), [2004]() ]; [ [Mussa-Ivaldi & Bizzi](), [2000] ]。 
在机器人学领域，基于动力学系统的模型已被用于构建用于连续控制的动态运动基元 (DMP)[ [Ijspeert et al.](), [2003]() ]; [ [Schaal](), [2006]() ]。 
模仿学习也可以用于学习 DMP，可以使用 RL 在线调整 [ [Kober & Peters](), [2009b;a]() ]。 
然而，这些与我们的工作有很大的不同，因为它们是为机器人任务专门参数化的，并且产生运动轨迹计划的编码，而不是动作的编码。

后来，[ [Thomas & Barto](), [2012]() ] 展示了如何使用仅控制潜在控制问题的有用的子空间的多个运动基元来学习目标条件策略。 
为了学习二进制运动基元，[ [Thomas & Barto](), [2011]() ] 展示了如何将一项策略建模为多个“协智能体” (coagents) 的组合，每个协智能体只使用局部的策略梯度信息进行学习 [ [Thomas](), [2011]() ]。 
我们的工作遵循类似的方向，但我们专注于自动学习离散动作的最优连续值动作表示。 
对于动作表示，我们提出了一种使用监督学习的方法，并限制使用高方差策略梯度仅训练内部策略。

**Other Domains**
其他领域

在有监督学习中，输出类别的表示已用于提取标签之间的附加相关性信息。 
流行的例子包括用于图像分类的学习标签嵌入 [ [Akata et al.](), [2016]() ] 和用于自然语言问题的学习单词嵌入 [ [Mikolov et al.](), [2013]() ]。 
相反，对于 RL 设置，策略是其输出对应于可用动作的函数。 
我们展示了学习动作表征也是有益的。

## 4. Generalization over Actions

在强化学习中，捕获 MDP 底层状态空间中的结构的好处是一个众所周知的概念，也是一个广泛使用的概念。 
状态表示允许策略跨状态泛化，那么类似地，在动作空间中通常存在另外的结构可以利用。 
本文假设，利用这种结构可以实现跨动作的快速泛化，从而使使用大型动作集进行学习是可行的。 
为了弥补这一差距，本文引入了动作表示空间 $\mathcal{E}\subseteq\mathbb{R}^d$，并考虑了由嵌入 (Embedding) 到动作映射函数 $f：\mathcal{E}\rightarrow \mathcal{A}$ 和内部策略 $\pi_i：\mathcal{S}\times\mathcal{E}\rightarrow [0,1]$ 参数化的可分解策略 $\pi_o$，使得给定 $S_t$ 的 $A_t$ 的分布被表征为：
$$
E_t\sim \pi_i(\cdot|S_t)\\
A_t = f(E_t)
$$ 其中 $\pi_i$ 是用于采样动作表示 $E_t\in\mathcal{E}$，并且函数 $f$ 将该表示确定地映射到集合 $\mathcal{A}$ 中的动作。 
这两部分一起形成了总体策略 $\pi_o$。 
**图 2** 说明了在这样的参数化下每个动作的概率。 
稍微滥用一下记号，本文使用 $f^{−1}(a)$ 作为一对多函数，其表示由函数 $f$ 映射到动作 $a$ 的表示集，即 $f^{−1}(a):=\{e\in\mathcal{E}：f(e)=a\}$。

在接下来的章节中，文章将讨论最优策略 $\pi^∗_o$ 的存在性以及 $\pi_o$ 的学习过程。 
为了阐明 (alucidate) 所涉及的步骤，我们将其分为四个部分。

1. 证明存在 $f$ 和 $\pi_i$ 使得 $\pi_o$ 是最优策略。
2. 给出当 $\pi_i$ 固定时函数 $f$ 的监督学习过程。
3. 给出当 $f$ 固定时 $\pi_i$ 的策略梯度学习过程。
4. 将以上过程结合起来，同时学习 $f$ 和 $\pi_i$。

### 4.1 Existence of $\pi_i$ and $f$ to Represent An Optimal Policy

本节的目的是建立一个条件使得在这个条件下，$\pi_o$ 可以表示最优策略。 
然后使用所提出的参数化方法定义 $\pi_o$ 和 $\pi_i$ 的最优集合。 
为了建立主要结论，本文首先从必要的假设开始。

动作的特征可以自然地与它们如何影响状态转移相关联。 
为了学习到能捕捉这种结构的动作表示，本文考虑了通常用于学习概率图模型的标准马尔可夫属性 [ [Ghahramani](), [2001]() ]，并做出以下假设，即转移信息可以被充分编码以推断执行的动作。

**假设 A1**：给定一个嵌入 $E_t$，那么 $A_t$ 条件依赖于 $S_t$ 和 $S_{t+1}$:
$$
P(A_t|S_t,S_{t+1})=\int_{\mathcal{E}}P(A_t|E_t=e)P(E_t=e|S_t,S_{t+1})de.
$$

**假设 A2**：给定嵌入 $E_t$，那么 $A_t$ 是确定性的且由 $f:\mathcal{E}\rightarrow\mathcal{A}$ 表示，即存在 $a$ 使得 $P(A_t=a|E_t=e)=1$

现在建立一个必要条件，在这个必要条件下，本文提出的策略可以代表最优策略。 此条件在之后导出学习规则时也有用。

**引理 1.** 在**假设 A1 A2** 下，存在一个 $\pi_i$ 使得
$$
v^{\pi}(s)=\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)}\pi_i(e|s)q^{\pi}(s,a)de.
$$

---

**证明：**
对于任意 MDP $\mathcal{M}$，在策略 $\pi$ 下贝尔曼方程形式如下：
$$
\begin{aligned}
v^{\pi}(s)&=\sum_{a\in\mathcal{A}}\pi(a|s)q^{\pi}(s,a)\\
&=\sum_{a\in\mathcal{A}}\pi(a|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;|s,a)[\mathcal{R}(s,a)+\gamma v^{\pi}(s&#39;)]
\end{aligned}
$$ 用 $G$ 表示 $[\mathcal{R}(s,a)+\gamma v^{\pi}(s&#39;)]$，重新排列方程里的项。
$$
\begin{aligned}
v^{\pi}(s)&=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\pi(a|s)\frac{P(s&#39;,s,a)}{P(s,a)}G\\
&=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\pi(a|s)\frac{P(s&#39;,s,a)}{\pi(a|s)P(s)}G\\
&=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\frac{P(s&#39;,s,a)}{P(s)}G\\
&=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\frac{P(a|s,s&#39;)P(s&#39;,s)}{P(s)}G
\end{aligned}
$$ 使用 law of total probability，引入一个新变量 $e$ 使得：
$$
v^{\pi}(s)=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\int_e\frac{P(a,e|s,s&#39;)P(s,s&#39;)}{P(s)}G\text{d}e\\
$$ 分式上下同时乘以 $P(e|s)$，得到
$$
\begin{aligned}
v^{\pi}(s)&=\sum_{a\in\mathcal{A}}\sum_{s&#39;\in\mathcal{S}}\int_e P(e|s)\frac{P(a,e|s,s&#39;)P(s,s&#39;)}{P(e|s)P(s)}G\text{d}e\\
&=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}\frac{P(a,e|s,s&#39;)P(s,s&#39;)}{P(s,e)}G\text{d}e\\
&=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}\frac{P(a,e,s,s&#39;)}{P(s,e)}G\text{d}e\\
&=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;,a|s,e)G\text{d}e\\
&=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;|s,a,e)P(a|s,e)G\text{d}e\\
\end{aligned}
$$ 因为转移到下一个状态 $S_{t+1}$ 条件独立于 $E_t$，所以
$$
v^{\pi}(s)=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;|s,a)P(a|s,e)G\text{d}e
$$ 类似地，使用马尔可夫性，动作 $A_t$ 条件独立于 $S_t$，所以
$$
v^{\pi}(s)=\sum_{a\in\mathcal{A}}\int_e P(e|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;|s,a)P(a|e)G\text{d}e
$$ 根据**假设 A2**，$P(a|e)$ 从 $e$ 映射到 $a$ 的概率为 1，其他为 0，那么
$$
\begin{aligned}
v^{\pi}(s)&=\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)} P(e|s)\sum_{s&#39;\in\mathcal{S}}P(s&#39;|s,a)G\text{d}e\\
&=\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)} P(e|s)q^{\pi}(s,a)\text{d}e
\end{aligned}
$$ 注意到 $P(e|s)$ 是内部策略 $\pi_i(e|s)$，因此得到：
$$
v^{\pi}(s)=\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)}\pi_i(e|s)q^{\pi}(s,a)de.
$$
**证明完毕**

---

根据**引理 1** 可以定义整体策略如下：
$$
\pi_o(a|s):=\int_{f^{-1}(a)}\pi_i(e|s)de.
$$

**定理 1.** 在**假设 A1 A2** 下，存在整体策略 $\pi_o$ 使得 $v^{\pi_o}=v^*$

Proof:

这一定理可直接由**引理 1** 引出。
因为状态集和动作集是有限的，奖励是有界的，并且$\gamma\in [0,1)$，所以至少存在一个最优策略。 
对于任何最优策略 $\pi^*$，对应的状态值函数和状态动作值函数是唯一的 $v^*$ 和 $q^*$。

通过**引理 1**，存在 $f$ 和 $\pi_i$ 使得
$$
v^*(s)=\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)}\pi_i(e|s)q^*(s,a)de.
$$

因此，存在 $\pi_i$ 和 $f$，使得组成得到的 $\pi_o$ 具有状态值函数 $v^{\pi_o}=v^*$，因此它表示最优策略。

注意，**定理 1** 基于状态值函数的等价性建立了最优总体策略的存在性，但不确保所有最优策略都可以由总体策略来表示。 【这是为什么？】
利用上式，我们定义了 $\Pi_o^*:=\{\pi_o:v^{\pi_o}=v^*\}$。 
相应地，我们将最优内部策略集定义为
$$
\Pi_i^*:=\{\pi_i:\exists \pi_o^*\in \Pi_o^*,\exists f,\pi_o^*(a|s)=\int_{f^{-1}(a)}\pi_i(e|s)de\}
$$

### 4.2 Supervised Learning of $f$ For a Fixed $\pi_i$

**定理 1** 证明了存在 $\pi_i$ 和函数 $f$，有助于预测从 $S_t$ 到 $S_{t+1}$ 的转移，从而使得相应的整体策略是最优的。
然后这样的函数 $f$ 可能不是先验已知的。
本节介绍一种使用从与环境交互中收集的数据来估计 $f$ 的方法。

由**假设 A1 A2**，$P(A_t|S_t,S_{t+1})$ 可以用 $f$ 和 $P(E_t|S_t,S_{t+1})$ 表示。 
本文提出寻找 $f$ 的估计量 $\hat{f}$ 和 $P(E_t|S_t,S_{t+1})$ 的估计量 $\hat{g}(E_t|S_t,S_{t+1})$，使得 $P(A_t|S_t,S_{t+1})$ 的重构是精确的。 
设基于 $\hat{f}$ 和 $\hat{g}$ 的 $P(A_t|S_t,S_{t+1})$ 的估计为 
$$
\hat{P}(A_t|S_t,S_{t+1})=\int_{\mathcal{E}}\hat{f}(A_t|E_t=e)\hat{g}(E_t=e|S_t,S_{t+1})de.
$$

度量 $P(A_t|S_t,S_{t+1})$ 和 $\hat{P}(A_t|S_t,S_{t+1})$ 之间的差异的一种方法是使用预期的 (在来自 on-policy 分布的状态上) **Kullback-Leibler** (KL) 散度：
$$
\begin{aligned}
&=-E[\sum_{a\in \mathcal{A}}P(a|S_t,S_{t+1})\ln(\frac{\hat{P}(a|S_t,S_{t+1})}{P(a|S_t,S_{t+1})})]
\\&=-E[\ln(\frac{\hat{P}(A_t|S_t,S_{t+1})}{P(A_t|S_t,S_{t+1})})]
\end{aligned}
$$

由于观察到的转移元组 $(S_t,A_t,S_{t+1})$ 包含对给定的 $S_t$ 到 $S_{t+1}$ 转移负责的动作，因此可以使用上式来计算 KL 散度的 on-policy 样本估计。 
基于 $P(A_t|S_t,S_{t+1})$ 和 $\hat{P}(A_t|S_t,S_{t+1})$ 之间的 KL 散度，采用如下损失函数：
$$
\mathcal{L}(\hat{f},\hat{g})=−E[\ln (\hat{P}(A_t|S_t,S_{t+1})]
$$

其中 (4) 中的分母不包含在 (5) 中，因为它不依赖于 $\hat{f}$ 或 $\hat{g}$。
如果$\hat{f}$ 和 $\hat{g}$ 是参数化的，则可以使用有监督的学习过程通过最小化损失函数 $\mathcal{L}$ 来学习它们的参数。

该模型的计算图如**图 3** 所示。
参阅**附录 D** 了解实验中使用的 $\hat{f}$ 和 $\hat{g}$ 的参数化。

---

**附录 D.1.** 参数化
在本文实验中，考虑一个最小化算法计算复杂度的参数化。 
学习动作表示模块的参数，如公式 (5) 所示 ，需要计算 (3) 中的值 $p (a | s，s&#39;)$。 
这涉及到 $e$ 上的一个完全积分。 
由于没有任何封闭形式的解，所以需要依靠随机估计。 
根据 $e$ 的维数，一个基于样本的广泛的期望评估可能计算代价昂贵。
为了使这个问题更容易处理，使用 $\hat{g}$ 的平均值的估计来近似 (3)。 也就是说，近似 (3) 为 $\hat{f} (a|\hat{g} (s,s&#39;))$。 
然后将 $\hat{f} (a | g (s,s&#39;))$ 参数化为，
$$
\hat{f}(a|\hat{g}(s,s&#39;))=\frac{e^{z_a/\tau}}{\sum_{a&#39;}e^{z_{a&#39;}/\tau}}
$$ 其中，$z_a= W^T_a \hat{g} (s,s&#39;)$。 
这个估计量 $\hat{f}$，根据它与给定表示 $e$ 的相似性来模拟任何动作 $a$ 的概率。在上式中，$W \in \mathbb{R}^{d_e\times|\mathcal{A}|}$ 是一个矩阵，其中每一列表示 $\mathbb{R}^{d_e}$ 维的可学动作表示。 $W_a^T$ 是对应于动作 $a$ 的表示的向量的转置，$z_a$ 是它与 $\hat{g} (s,s&#39;)$ 嵌入相似性的度量。 
为了得到有效的概率值，波兹曼分布被用作一个温度变量 $\tau$。
在 $\tau\rightarrow0$ 的极限下，动作上的条件分布成为 $f$ 的必要的确定性估计。也就是说，整个概率质量都在动作 $a$ 上，$a$ 的表示与 $e$ 最相似。 
为了确保训练过程中的经验稳定性，我们把 $\tau$ 放松到1。 
在执行过程中，将选择具有与 $e$ 最相似的表示的动作 $a$ 来执行。
实际上，上式中的线性分解是不受限制的，因为 $\hat{g}$ 仍然可以是任意的可微函数近似器，就像神经网络一样。

---

请注意，虽然 $\hat{f}$ 将用于整体策略中的 $f$，但 $\hat{g}$ 仅用于查找 $\hat{f}$，并不会用于其他目的。

由于这种有监督的学习过程只需要估计 $P(A_t|S_t,S_{t+1})$，因此它不需要 (或依赖于) 奖励。 
这部分地缓解了由于稀疏和随机奖励引起的问题，因为替代的信息监督信号总是可用的。这对于使整个策略的动作表示组件快速学习并且具有低方差更新是有利的。【这是为什么？】

### 4.3 Learning $\pi_i$ For a Fixed $f$

学习用 $\theta$ 参数化的策略的常见方法是优化带有折扣的开始状态目标函数 $J(\theta):=\sum_{s\in S}d_0(s)v^{\pi}(s)$。
对于权重为 $\theta$ 的策略，可以通过增加策略梯度 $\frac{\partial J(\theta)}{\partial \theta}$ 来提升策略的期望性能。

设与内部策略 $\pi_i$ 相关的状态值函数为 $v^{\pi_i}(s)=E[\sum_{t=0}^\infty \gamma^t R_t|s,\pi_i,f]$，且状态动作值函数 $q^{\pi_i}(s,e)=E[\sum_{t=0}^\infty \gamma^t R_t|s,e,\pi_i,f]$。
然后，我们将 $\pi_i$ 的性能函数定义为:
$$
J_i(\theta):=\sum_{s\in\mathcal{S}}d_0(s)v^{\pi_i}(s)
$$

将嵌入视为具有策略 $\pi_i$ 的智能体的动作，策略梯度定理 [ [Sutton et al.](), [2000]() ] 指出 (6) 的无偏 [ [Thomas](), [2014]() ] 梯度为 
$$
\frac{\partial J_i(\theta)}{\partial \theta}=\sum_{t=0}^\infty E[\gamma^t\int_{\mathcal{E}}q^{\pi_i}(S_t,e)\frac{\partial}{\partial \theta}\pi_i(e|S_t)de]
$$ 其中，如 [ [Sutton et al.](), [2000]() ] 所定义的，期望是在来自 $d^\pi$ 的状态上的期望。(这不是真实分布，因为它没有正规化)。

内部策略的参数可以通过在 $\partial J_i(\theta)/\partial\theta$ 方向上迭代更新其参数来学习。
由于对策略 $\pi_i$ 没有特殊限制，因此可以使用为连续控制而设计的任何策略梯度算法，例如 DPG [ [Silver et al.](), [2014]() ]、PPO [ [Schulman et al.](), [2017]() ]、NAC [ [Bhatnagar et al.](), [2009]() ] 等。

然而，注意，与总体策略 $\pi_o$ 相关联的性能函数 (由函数 $f$ 和用权重 $\theta$ 参数化的内部策略组成) 是：
$$
J_o(\theta,f)=\sum_{s\in\mathcal{S}}d_0(s)v^{\pi_o}(s)
$$ 最终要求是改善这个整体性能函数 $J_o(\theta, f)$，而不仅仅是 $J_i(\theta)$。 
那么，通过跟踪其自身性能函数的梯度来更新内部策略 $\pi_i$ 有多大用处呢？

下面的引理回答了这个问题。

**引理 2.** 对于将表示空间中的每个点 $e\in \mathbb{R}^d$ 映射到动作 $a\in\mathcal{A}$ 的所有确定性函数 $f$，基于 $\frac{\partial J_i(\theta)}{\partial \theta}$ 的参数 $\theta$ 期望更新等价于基于 $\frac{\partial J_o(\theta, f)}{\partial \theta}$ 的更新。 即
$$
\frac{\partial J_o(\theta, f)}{\partial \theta}=\frac{\partial J_i(\theta)}{\partial \theta}
$$

---

**证明**

使用**引理 1**，将 $\pi_o$ 的性能函数重写如下：
$$
\begin{aligned}
J_o(\theta,f)&=\sum_{s\in\mathcal{S}}d_0(s)v^{\pi_o}(s)\\
&=\sum_{s\in\mathcal{S}}d_0(s)\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)}q^{\pi_o}(s,a)\text{d}e.
\end{aligned}
$$
对上式求梯度
$$
\frac{\partial J_o(\theta,f)}{\partial \theta}=\frac{\partial}{\partial \theta}[\sum_{s\in\mathcal{S}}d_0(s)\sum_{a\in\mathcal{A}}\int_{f^{-1}(a)}q^{\pi_o}(s,a)\text{d}e.]
$$ 对整体策略 $\pi_o$ 使用 [ [Sutton et al.](), [2000]() ] 的策略梯度定理，则
$$

$$
注意到 $e$ 是确定性的映射到 $a$，$q^{\pi_o}(S_t,a)=q^{\pi_i}(S_t,e)$，所以
$$$$
最后因为每一个 $e$ 都通过函数 $f$ 映射到唯一的动作，那么关于 $a$ 的 nested 求和以及内部积分可以替换为在整个 $e$ 上的积分，因此
$$

$$
**证明完毕**

---

将证明推迟到**附录 B**。
为策略选择的参数化具有这样的特殊性质，这允许使用其内部策略梯度来学习 $\pi_i$。
由于该梯度更新不需要显式计算任何 $\pi_o(a|s)$ 的值，因此可以避免 $\pi_o$ 所需的(1) 中的 $f^{-1}$ 的潜在难以处理的计算。
相反，可以直接使用 $\partial J_i(\theta)/\partial \theta$ 来更新内部策略的参数，同时仍然优化整体策略的性能 $J_o(\theta,f)$。

### 4.4 Learning $\pi_i$ and $f$ Simultaneously

由于 $f$ 的监督学习过程不需要奖励，所以只需要一些包含足够的信息的初始轨迹来开始学习有用的动作表示。
随着更多的数据变得可用，它可以通过微调并改进动作表示。

#### 4.4.1 Algorithm

本文算法称为带有动作表示的策略梯度 Policy Gradients with Representations 佛如 Actions (PG-RA)。 
PG-RA 首先通过使用随机策略采样几个轨迹并使用 (5) 中定义的监督损失来初始化动作表示组件中的参数。 
如果如先前的工作[ [Dulac-Arnold et al.](), [2015]() ] 中假设的那样，关于动作的附加信息已知，则在初始化动作表示时也可以考虑这些信息。 
可选的是，一旦这些动作表示被初始化，它们就可以保持固定。

在**算法 1** 中，第 2-9 行说明了所有涉及的参数的在线更新过程。 
Episode 的每个时间步长由 $t$ 表示。 
对于每一步，对动作表示进行采样，然后由 $\hat{f}$ 将其映射到动作。 
在环境中执行此动作后，观察到的奖励用于使用任意策略梯度算法更新内部策略 $\pi_i$。 
根据策略梯度算法，如果使用了 critic，则使用TD-Error 的半梯度用于更新 critic 的参数。 
在其他情况下，比如在 REINFORCE [ [Williams](), [1992]() ] 中，没有 critic，这一步可以忽略。
然后，在第 9 行中使用观察到的状态转移来更新 $\hat{f}$ 和 $\hat{g}$ 的参数，以便最小化有监督的学习损失 (5)。 
在我们的实验中，第9行使用随机梯度更新。

#### 4.4.2 PG-RA Convergence

如果在学习内部策略的同时动作表示保持固定，那么作为**Property 2** 的结果，本文的算法的收敛性直接满足之前的两个时间尺度 (two-timescale) 的结果[ [Borkar & Konda](), [1997]() ]; [ [Bhatnagar et al.](), [2009]() ]。 
在这里，我们表明，使用 PG-RA 算法同时学习 $\pi_i$ 和 $f$ 也可以通过使用三时间尺度 (three-timescale) 分析来证明收敛。

类似于先前的工作 [ [Bhatnagar et al.](), [2009]() ]; [ [Degris et al.](), [2012]() ]; [ [Konda & Tsitsiklis](), [2000]() ]，为了分析对内部策略 $\pi_i$ 的参数 $\theta\in\mathbb{R}^d$ 的更新，使用投影运算符 $\Gamma:\mathbb{R}^{d_{\theta}}\rightarrow \mathbb{R}^{d_{\theta}}$，其将任何 $x\in\mathbb{R}^{d_{\theta}}$ 投影到紧集 $\mathcal{C}\subset \mathbb{R}^{d_{\theta}}$。 
然后，我们定义了一个相关的向量域运算符 $\hat{\Gamma}$，它将紧凑区域 $\mathcal{C}$ 之外的任何梯度投影回 $\mathcal{C}$。 
关于这些运算符的精确定义和附加的标准假设 (A3)-(A5)，请读者参阅**附录 C.3**。 
然而，实际上，我们不会将迭代投影到约束区域，因为它们看起来是有界的(没有投影)。

**定理 2.** 在假设 (A1)-(A5) 下，内部策略参数 $\theta_t$，当 $t\rightarrow\infty$ 时，以概率 1 收敛到 $\hat{Z}=\{x\in\mathcal{C}|\hat{\Gamma}(\frac{\partial J_i(x)}{\partial \theta})=0\}$

Proof:
(Outline) 我们考虑三个学习速率序列，使得内部策略的更新递归在最慢的时间尺度上，critic 的更新递归在最快的时间尺度上，而动作表示模块的更新递归在中等速率上。 
通过这种结构，利用三时间尺度分析技术 [ [Borkar](), [2009]() ] 并证明了收敛性。 完整的证明在**附录 C** 中。

## 5. Empirical Analysis

这项工作的一个核心动机是提供一个算法，该算法可以作为一个插入式扩展 (drop-in extension)，用于提高现有策略梯度方法对大动作空间问题的动作泛化能力。 
在我们的实验中，我们考虑了两种标准的策略梯度方法：Actor-Critic (AC) 和确定性策略梯度 (DPG) [ [Silver et al.](), [2014]() ]。 
就像以前的算法一样，我们也忽略了 $\gamma^t$ 项，执行有偏策略梯度更新，以便实际上更具样本效率 [ [Thomas](), [2014]() ]。 
我们相信，将所提出的方法与其他策略梯度方法结合使用，可以进一步改善已发表的结果；
我们将这一点留到未来的工作中去做。 
有关函数逼近器的参数化和超参数搜索的详细讨论，请参见**附录 D**。

### 5.1 Domains

**Maze**

作为概念验证 (proof-of-concept)，我们构建了一个连续状态迷宫环境，其中状态由智能体当前位置的坐标组成。
智能体周围有 $n$ 个等间距的执行器 (actuators) (每个执行器沿执行器指向的方向移动智能体)，并且它可以选择每个执行器应该打开还是关闭。 
因此，动作集的大小与执行器的数量呈指数关系，即 $|\mathcal{A}|=2^n$。
动作的净结果 (net outcomes) 是与所选执行器相关的位移的矢量总和。
智能体每走一步都会得到一个小的惩罚项，到达目标位置后会得到 100 的奖励。 
为了使问题更具挑战性，在动作中增加了 10% 的随机噪声，最大 episode 长度为 150 步。

此环境是一个有用的试验台，因为它需要在具有大动作集和单一目标奖励的 MDP 中解决长期任务。 
此外，我们知道每个动作的笛卡尔 (Cartesian) 表示法，因此可以使用它来可视化学习到的表示法，如**图 4** 所示。

**Real-world recommender systems**

我们考虑推荐系统的两个实际应用，这两个应用需要在多个时间步骤上做出决策。
首先，一个基于网络的视频教程平台，它有一个推荐引擎，可以推荐各种软件上的一系列教程视频。 
其目的是让用户有意义地学习如何使用这些软件，并将新手用户转变为各自感兴趣领域的专家。
每个时间步的教程建议是从几个软件上的大量可用教程视频池中制作出来的。

第二个应用是专业的多媒体编辑软件。 
现代多媒体编辑软件通常包含许多可用于操纵媒体的工具，这些丰富的选项可能会让用户不知所措。 
在这个领域中，智能体将建议用户下一步可能要使用哪些可用工具。 
目标是提高用户工作效率并帮助实现他们的最终目标。

对于这两个应用，使用用户点击流数据的现有日志来创建用于用户行为的基于 n-gram 语法的 MDP 模型 [ [Shani et al.](), [2005]() ]。 
在教程推荐任务中，观察了为期三个月的用户活动。 
聚合用户交互序列以获得超过 2900 万次点击。 
类似地，在长达一个月的持续时间内，收集了多媒体编辑软件中工具的连续使用模式，获得了总计超过 17.5 亿次的用户点击。 
总共点击次数少于 100 次的教程和工具将被丢弃。 
其余的 1498 个教程和 1843 个工具分别用于基于 Web 的教学平台和多媒体软件，用于创建 MDP 模型的动作集。 
MDP 具有连续的状态空间，其中每个状态由与当前 n-gram 语法中的每个项目 (教程或工具) 相关联的特征描述符组成。 
奖励的选择分别基于对多媒体编辑软件中用户交互的用户交互结果的难易程度和受欢迎程度的替代测量。 
由于这样的数据很稀疏，只有 5% 的物品有与之相关的奖励，任何物品的最高奖励都是 100。

推荐问题通常被表述为上下文强盗 (contextual bandit) 或合作过滤 (collaborative filtering) 问题，但正如 [ [Theocharous et al.](), [2015]() ] 所表明的那样，这些方法未能捕捉到预测的长期价值。 
通过大量的动作 (教程/工具) 在更长的时间范围内解决这个问题，使得这个现实生活中的问题成为 RL 算法的一个有用和具有挑战性的领域。

### 5.2 Results

**Visualizing the Learned Action Representations**

为了理解我们所提出的算法的内部工作原理，我们在迷宫域上给出了学习的动作表示的可视化表示。
**图 4** 提供了环境的图示。这里，动作集合中的底层结构与笛卡尔坐标中的位移相关。 
这提供了一个直观的基础情况，我们可以将其与我们的结果进行比较。

在**图 4** 中，我们提供了使用我们的算法学习的动作表示与动作的底层笛卡尔表示之间的比较。 
由此可见，该方法提取了动作空间中的有用结构。 
对应于选择智能体相反侧上的执行器的设置的动作导致对智能体的相对较小的位移。这些就是处于图中心的那些。 
相反，任何方向上的最大位移是由仅选择面向该特定方向的执行器引起的。 
与这些相对应的动作位于表示空间的边缘。 
平滑的颜色过渡表明，不仅可以表示位移的大小信息，还可以表示位移的方向。 
因此，所学习的表示有效地保存了所有动作之间的相对过渡信息。 
为了使探索步骤在内部策略 $\pi_i$ 中易于处理，我们使用 tanh 非线性函数将沿着每个维度的表示空间限制到范围 [−1，1]。 
这会导致这些表示在此范围边缘附近被“挤压” (squashing)。

**Performance Improvement**

**图 5** 中迷宫领域的图表显示了标准 Actor-Critic (AC)方法的性能如何随着动作数量的增加而恶化，即使目标保持不变。 
但是，通过添加动作表示模块，它能够捕获动作空间中的底层结构，并在所有设置中保持良好的性能。 
同样，对于教程和软件 MDP，标准的 AC 方法在如此多的动作下无法在更长的时间范围内进行推理，选择的大多是具有高回报的一步动作。 
相比较而言，我们提出的算法实例不仅能够在各自的任务中获得高达 2 倍和 3 倍的高回报，而且它们的执行速度要快得多。 
这些结果强化了我们的主张，即学习动作表征允许将反馈隐含地概括到嵌入在执行动作附近的其他动作。

此外，在 PG-RA 算法下，使用高方差策略梯度更新只学习内部策略中的一小部分总参数。 
通过有监督的学习过程来学习与动作表示相关联的另一组参数。 
这显著降低了更新的方差，从而使 PG-RA 算法更快地学习到更好的策略。 
从**图 5** 中的图表可以明显看出这一点。
这些优势允许内部策略 $\pi_i$ 快速逼近最优策略，而不会屈服于大型动作集的诅咒 (without succumbing to the curse of large actions sets)。

## 6. Conclusion

在这篇文章中，我们建立在动作空间中利用结构的核心思想，并展示了它对于在现实世界大规模应用中增强对大型动作集的泛化的重要性。

我们的方法有三个关键优势。

(A) 简单性：通过简单地使用观察到的转换，可以使用附加的监督更新规则来学习动作表示。

(B) 理论：我们证明了所提出的总体策略类可以表示最优策略，并推导了其参数的相关学习过程。

(C) 可扩展性：正如 PG-RA 算法所表明的那样，我们的方法可以使用其他策略梯度方法轻松扩展，以利用额外的优势，同时保持收敛保证。
