今天分享的这一篇是 2020 年 ICLR 的论文，在高维问题中学习规划。这个神经探索利用树，就是本文提出的算法 NEXT。先看摘要

文章提出了一种名为神经探索-利用树（Neural Exploration-Exploitation Trees，简称NEXT）的元路径规划算法，用于在高维连续状态空间和动作空间中求解新的路径规划问题。与更多经典的基于采样的方法（如RRT）相比，在高维空间中实现了更高的采样效率，并且可以从类似环境中的先验规划经验中受益。更具体地说，NEXT利用了一种新型的神经结构，可以从问题结构中学习有前景的搜索方向。然后将学习到的先验经验整合到UCB型算法中，在解决一个新问题时，实现了探索和利用之间的在线平衡。进行了充分的实验，表明NEXT以更紧凑的搜索树完成了新的规划问题，并在一些基准上明显优于最先进的方法。

Introdcution 开头抛出要解决的问题，路径规划。在低维情形通过把连续状态空间离散化然后用图搜索方法就能获得不错的解。但由于维数灾难，这些方法不能很好的应用到高维情形。对于高维规划问题，通常采用基于采样的方法，例如 PRM,RRT等等，他们都是在采样的同时增量式地建立起空间的隐表示。他们通常采用的是均匀采样的方式，没有对问题的结构进行针对性的调整采样方式，当然也有人进行针对性采样的做法，但人为设计的采样方式无法泛化到不同的问题上。

除此之外在线自适应路径规划也有不少的研究。但是这些研究对于每个规划问题都是独立求解的，而且以往使用的数据和建立的模型往往在求解新问题的时候会抛弃，而相似的问题可能会不断的遇到，它们虽然不同但可能拥有一致的结构，这个想法就是学习求解问题也就是规划本身的潜在结构的motivation。这会使得规划方法具有更好的泛化能力，提高样本效率。

当然也有人尝试做高样本利用率，泛化能力强的方法，大部分还依赖于特别设计的局部特征或假设问题具有特别的参数化形式。总而言之，现有方法还不够好。

那么文章提出 神经探索利用树 NEXT，一个用于高维连续状态空间问题的元神经路径规划算法，其核心是基于注意力的神经架构，用于学习可泛化的问题结构。利用在线探索利用平衡自适应，来为搜索方向提供指导。

和现有的方法相比，其通用性更强，具有控制探索利用的平衡的能力。

第二节形式化了我们的路径规划问题。把机器人看成一个刚体，其位姿可以由若干个相互独立的参数组成的向量表示，这个向量可以唯一确定机器人在工作空间的状态，这些向量的集合称为构形空间或者位形空间或者C空间一般是【0,1】区间。C空间中有障碍物空间Sobs，那么自由空间就是状态空间去掉障碍空间，初始点和目标点都在自由空间中。那么路径规划问题就可以描述成一个三元组也就是在自由空间中找到一条从起始点到目标点的路径，这个路径可以表示为\Xi 将【0，1】映射回原空间，如果这个路径完全属于自由空间那称为无碰撞路径，如果从起点到终点那称为可行路径。那么工作空间W中的规划任务就可以转化为构形空间中路径规划问题。

最优路径规划问题就是找到可行范围内成本最低从起点到目标的最优路径。往往因为这个可行范围也就是s free有非常不规则的几何形状，所以通常用一个碰撞检测模块来判断路径是否遇见了障碍物，而可选路径函数也很难参数化所以分成T段路径进行处理。这样就是一个完整的路径规划问题，那么本文要做的就是根据规划过的问题来学习规划这个行为。也就是说从一个规划算法族中选择在采样得到的问题集上表现最好的算法。这个评价方式就是损失函数，用于评价算法给出问题的对应解的好坏或者是算法的效率。从公式二我们可以看到要做的几件事情。首先是问题的采样方式，其次是算法族的创建，最后是损失函数的定义。分别对应第三节，第四节4.1 4.2，第四节4.3.

第三节给出了一个基于树的采样算法的统一视角，称为 TSA 算法。对于一个问题 U，初始化一颗树，顶点集初始化为起点，边集为空。因为我们把路径分成了T段，所以对于每一段，我们使用扩展运算，扩展实际分为两步，先从树T中选择一个节点作为父节点，然后在这个节点的邻域采样得到新节点。那么有了两个点连成的路径段，先进行障碍物的判断，如果路径段上没有障碍物即无碰，那么更新顶点集和边集。第七行是可选的，有的算法会对树进行后处理，这个后处理我也不太清楚，然后如果选出的新状态达到了目标区域那就返回搜索树。作者认为现有的TSA算法都是独立地从零开始处理规划问题，忽视了在相似环境下的规划经验。那么如何克服这一点？主要修改的地方就是 Expand 操作。

第四节就介绍了一种可学习的基于神经网络的Expand操作。前面说了Expand是两步走的，其一是选择节点，其二是选择该节点邻域内的一个节点进行扩展。那么第一步如何选择节点，文章将这一部分建模为上下文多臂老虎机，以前介绍过的对于这类问题常用的方法就是置信上界UCB算法，计算每个状态的置信上界然后选择最大的那个，本文的 UCB 则是 $\phi(s)$ ，其计算方式可以看到是平均奖励加上lambda控制的方差。【附录More Preliminaries】回忆一下UCB，他是分成两个部分，一个是对于臂i估计的期望值，这部分表示利用，另一部分可以理解为臂i的标准差，作为探索部分。当你的探索次数很小时，那么后面一项的值比较大，被选中的可能性更高，而探索的次数足够多时整个式子其实主要由左边决定。【回到Guided】如果要直接使用 UCB显然是不合适的，因为状态的数量不断增加而且互相不独立。那么本文的做法是所有的节点值函数的估计，取负号作为奖励，这个值函数表示的意思是从当前状态到达重点的期望成本，然后将这些奖励进行核平滑估计作为状态的期望值，至于另一部分按照我的理解，在做选择时，分子为定值，然后分母越大即，s到其他状态的核距离越大则说明它在树中的度较小，即接近叶子节点，所以是访问频率低的状态，鼓励探索。这就是第一步

选择好父节点之后，在父节点的邻域内选择新状态，我们可以进行均匀采样然后UCB选择，但是这样就没有针对性，所以用一个策略来指导采样这一过程，也就是从策略pi中采样k个候选点，然后同样使用UCB。

那么这个Expand的关键就落在了值函数和策略上，一个用于定义奖励，一个用于指导采样

为了让学习到值函数和策略具有泛化能力，自然就想到对原来的高维空间进行embedding，图2展示了一个具有N连接的机器人，图2中所示是个四条边，但是用三个角度就能表示其状态，加上所处的位置一共是五维，这就是状态对应的特征向量了。

文章使用两个不同的子网络来学习注意力，上半部分是空间注意力集中于工作空间，输出一个d×d维的miu，为了方便卷积神经网络处理，它把位置变成了张量形式，另一部分是除了空间位置以外的特征，用全连接网络输出注意力向量，然后和空间注意力矩阵做元素对应相乘最后得到一个注意力张量。

然后把目标点，或者说目标区域的中心输入注意力模块，再把map这个表示可行域的也加入到注意力张量中进行卷积得到初始化值函数，然后根据值迭代的思想让值函数收敛，这时候就得到所有可行域最优状态值函数，不过这是在注意力张量也就是嵌入空间做的，要输出特定的状态对应的值还需要进行处理，所以把状态s进行embedding和状态值函数对应元素相乘得到psi然后分别经过两个网络输出值函数和策略。这个也是常见的做法。一共有四个权重W0是产生值函数初始值，W1是值迭代，W2W3是输出值函数和策略对应的网络参数。这些都是一起训练的，那么训练的目标是什么，可以看成几块，一个是策略一个是值函数再加个正则项。值得说的是数据的获取，我们前面的算法TSA是输入问题得到一颗搜索树，如果成功到达目标那么加入经验池，然后训练的时候取出这个问题和对应的搜索树，根据搜索树重构出一条成功路径，这条路径知道之后就知道对应的成本和顶点用于优化目标函数。还有一点是使用了RRT和NEXT的混合，然后降低RRT的权重最后变成NEXT，我觉得是在为收集成功数据考虑。

实验的话值得一提的是学习到的值函数和策略，这个图颜色越深表示其成本越高，箭头则表示了策略对搜索的指导。其次是消融实验，文章把Expand操作，基于注意力的网络结构和策略价值三个分别进行替换证明了NEXT的有效性离不开这几个部分。
