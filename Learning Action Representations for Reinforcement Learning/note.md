MDP 中的状态表示已经有很多研究了，但是今天要分享的这一篇文章提出要学习动作的表示，这篇文章发表在 ICML 2019

说要学习动作空间的表示，那么为什么要学习动作空间的表示？很多 baseline 比如说 Atari 游戏，其动作空间实际上是比较少的离散空间，除了离散空间还有一些则是性质比较好的连续空间，这些情形显然没什么必要进行动作表示学习，因为本身足够简单，信息量足够。但是有的时候动作空间比较大，比如说一个时间步里需要执行几千个动作，这时候就会想到在动作空间上做一些泛化，也已经有研究表明这种泛化可以显著加快学习速度。那么这篇文章就介绍了一种动作空间表示的学习方法，并且有相应的理论保证，能够和策略梯度方法共同使用。

看一下摘要，大多数 model-free RL 方法利用状态表示 (embedding) 进行泛化，但是这些做法要么忽略动作空间中的结构，要么假设该结构是先验的。 文章展示了如何将策略分解为在动作表示的低维空间中执行动作的组件也就是内部策略 pai_i 和将这些表示转换为实际动作的组件 f。 这些representation通过 允许智能体推断与已经采取的动作类似的动作输出来改进对大型的、有限的动作集的泛化。 文章给出了一种既能学习又能使用动作表示的算法，并给出了算法收敛的条件。在大规模的实际问题上验证了该方法的有效性。

Introduction，比较一般的信息就不再多说，这篇文章最关键的就是提出把原本的策略拆分成两个部分，internal policy 内部策略和把 action embedding 映射到原来动作的确定性函数 f。原有的策略称为 overall policy 整体策略。可以看到图 1 显示的框架图。文章提出的方法是一种通过观察得到的状态转移来学习动作集的底层结构，将学习动作表示也就是学习 f这个函数的问题变成监督学习问题而不是RL问题。监督学习方法往往比RL更快更可靠。文章剩下的部分第二节是RL的基本设置，介绍了MDP的定义，符号表示，状态值函数和状态动作值函数。第三节是相关工作，列举了分解动作空间，辅助任务，运动基元等等领域的工作，其实要说很相关也没有，区别还是比较大的。第四节就是本文的主要内容，在动作空间上泛化，第五节是实验，第六节是结论。

那么我们直接看第四节，首先从状态泛化联想到动作泛化，引入一个动作表示空间花体E，一个由embedding映射到动作空间的函数 f，内部策略 pai_i 是在动作表示空间里的策略，他们共同构成了overall policy。公式表示这一过程，首先根据内部策略选择动作embedding，然后用f将其映射回原来的动作空间。注意到 f 是一个 many-to-one 多对一的函数，因此其逆函数是 one-to-many。

图2 表示的是三个动作，在一维embedding空间的概率，横轴表示 e，纵轴表示概率，颜色区域表示 a=f(e),每个颜色和特定动作相关。也就是说 f^-1 定义了每个动作对应的embedding 集，为了阐明整个方法的过程，将其分为四步，对应四个小标题。

第一个证明存在 f 和 pai_i 使得 pai_o 是最优策略，第二个是给出了固定pai_i时f的监督学习过程，第三是给出固定f，内部策略的策略梯度学习过程，第四十将以上过程结合，同时学习f和pai_i

首先看 4.1 这一节是建立必要的假设，假设给了A1和A2，前者是说动作At和st st+1条件独立，就有了下面这个式子，条件独立体现在右边的第一项，假设A2则是说f是个确定性函数，在这两个条件下，引理1告诉我们存在pai i使得状态值函数等于这么个式子，附录A有对应的证明，证明的话不是很难，就是利用贝尔曼方程和全概率公式以及条件独立性，然后之前一直没有说这个overall policy的定义，公式1给出了这个定义。然后导出定理1，存在pai o使得对应的状态值函数等于最优的状态值函数。

4.2 节固定pai i然后监督学习得到 f，因为前一小节的定理1我们可以知道存在pai i和f 使得整体策略最优，然后我们借助假设A1A2，将A1里那个条件独立的等式进行估计，f有估计量f hat，P（E_t|S_t,S_t+1）有估计量g hat，然后我们要重构出P（At|S_t,S_t+1）,那么用两个估计量和假设1导出我们对PAt的估计量，即公式3要让这个估计量与真实概率尽可能接近，那么一种自然而然地想法就是使用KL散度，如公式4所示。可以看到第二行式子的分母与两个估计量f hat g hat无关，那么可以直接最小化分子部分也就是公式5即可。只要给出关于两个估计量的参数化形式即可，这个形式具体参见附录D，这个也算是文章的核心之一，不过不知道为什么没有放在正文。可以先看一下附录D【介绍参数化】图 3左侧给出了我们这个监督学习的过程，可以看到输入数据是 st at s t+1状态转移，然后输出 g 然后经过 f 得到a，这一过程的核心式子就是公式3，然后计算损失函数计算梯度反向传播。可以看到 f_hat在别的地方还有用，图3右侧，而ghat只在左图用于估计f别无他用，所以用完之后就可以不要了，除此之外可以看到这一个过程并不需要用到奖励，文章认为这样是一个优点，因为不需要等到奖励获取，在稀疏奖励的情形同样可以训练得到f，这样看起来这不就是个model-based么，奖励的作用我觉得还可以再考虑考虑。

4.3节是固定f，可以说训练好了之后，学习内部策略pai_i，一般用于衡量策略好坏的方式有几种，其中一种就是计算开始状态的目标函数，或者开始状态的状态值函数，然后根据这个目标函数进行梯度更新。公式6给出了内部策略的优化目标，别人的论文给出了无偏梯度估计，由于对pai_i本身没有限制，所以可以用各种PG方法如DPG PPO NAC等等，但是我们的最终目的是要优化overall policy而不仅仅是内部策略，所以我们要考虑内部策略的梯度更新和overall有什么关系，引理2告诉我们这俩梯度更新是一样的。证明可以看附录B，这个证明主要就是使用策略梯度定理。

4.4节同时学习内部策略和表示函数f，算法叫做 Policy gradient with representation for actions PG-RA，具体的做法是怎么样呢，首先使用随即策略采样几个轨迹通过公式5定义的监督损失来初始化函数f的参数，当然要是有什么先验信息自然可以加进去，初始化，之后随着数据量增加可以进行调整改进。算法本身算是很简洁了，初始化玩动作表示函数之后，遍历时间步，从内部策略里采样动作表示然后经过表示函数得到At，执行获得st+1和奖励rt，用PG方法 更新内部策略，然后更新critic，然后最小化损失函数L。

除了算法本身，文章给出了算法的收敛性证明，这一部分我还没弄明白，用的是三时间尺度分析方法，方法本身比较复杂，文章用的比较多别人的结论 2009年borkar这个是一本书来的。

第五节经验性分析，就是做具体的实验了。给了两类例子 Maze迷宫和real-world 真实世界推荐系统。只能说这个例子做起来还挺困难的。这篇文章的算法可以作为一个拓展，结合不同的 PG 方法，提高这些方法对于大型动作空间问题的泛化能力。实验考虑两种常用的 PG 方法，AC 方法和 DPG，实验环境 Maze 是一个连续状态迷宫环境，状态由笛卡尔坐标表示，智能体的周围有n个等间距的actuator执行器，可以看到图 4第一个图，星星表示重点，红点表示智能体，他在这个位置有十二个箭头也就是执行器，每个执行器可以选择开或者关，每一步的动作都是这些执行器的矢量和，显然每一步的可能动作就有 2^12 个，智能体每一个时间步都获得一个惩罚，然后到达终点奖励+100，为了让这个问题更加复杂，往动作里加10%的随机噪声，图4的第二个图是动作移动产生位移对应的笛卡尔坐标，如果选了比较多相反方向的执行器，那么对智能体的位移就比较小，那些位移大的就是只选择特定方向的执行器，这类动作就在空间的边缘。平滑的颜色过渡还表示了位移的方向，由于使用了非线性函数将范围限制到[-1,1]，所以边缘的点被挤压了

图五显示了标准AC方法的性能与动作数量的关系，上面一排从4个到8个再到12个执行器，比较了 AC 和加入本文算法的AC-RA，可以看到随着执行器数量增多，动作数量增多，标准AC的性能恶化的很快，而AC-RA仍然保持有相当的性能

第二个实验是推荐系统，一个是基于网络的视频教程平台，他有一个推荐引擎可以推荐各种软件的一系列教程视频。其目的是让用户有意义地学习如何使用软件并将新手变成各自感兴趣领域的专家，每个时间步的教程建议都是从大量可用教程视频池中导出的。

另一个应用是专业的多媒体编辑软件，就是说现代编辑软件有很多工具，这些工具可能让用户不知所措，那么智能体的任务是建议用户下一步可能使用的工具，目的是提高用户工作效率。

至于数据的收集，是用用户的点击流数据的现有日志来创建 N元 MDP 模型。N-gram是一种基于统计语言模型的算法，基本思想是将文本里的内容按照字节进行大小为N的滑动窗口操作，形成了长度为N 的字节片段序列，每一个片段称为 gram，这个模型是认为第N个词只和前面n-1个词相关，这个是NLP的一个基本模型用处挺多的。

第一个应用是观察了三个月的用户活动，聚合用户交互序列以获得超过2900万次点击，类似的第二个应用是收集了一个月内多媒体编辑软件总计超过17.5亿次用户点击，去掉那些点击数少于100次的。然后剩下的1498个教程和1843个工具用于创建动作集，具体的数据没有见到其实不是很清楚是怎么做的。奖励是用一个替代度量，表示用户交互结果的难易程度和受欢迎程度，状态则是由特征描述符组成，这一部分的实验结果在图5的第二行，AC-RA DPG-RA 和标准AC，不过没有 DPG有点奇怪不太好比较，两个不同的MDP对应的曲线，可以看到加了RA是要比AC更好的。

结论是本文方法利用了动作空间内部潜在的结构性质，也就是学习动作表示对大型动作集有重要意义，方法有三个关键优势：首先是简单，只使用了观察到的状态转移来训练动作表示，其次是理论保证，overall 可以表示最优策略并且推导了参数的相关学习过程，最后是可拓展，可以结合各种PG方法同时保证收敛。

那么未来的方向，我思考了一下，刚刚也说了没有奖励，但是奖励本身还是有用的，只是因为稀疏性等，比较难起到指导的作用，那么其实可以尝试一下多步的状态转移，让奖励不那么稀疏。算法在计算的时候需要对所有动作进行求和，这个对大型数据集应该还是有改进空间的。目前的话我还是想着用图描述MDP然后试试看动作的描述方式，如果可以的话大型图用图卷积学习到的表示和这种动作表示进行比较。
